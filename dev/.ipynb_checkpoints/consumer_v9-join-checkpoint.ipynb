{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell'  \n",
    "from pyspark import SparkContext  \n",
    "from pyspark.streaming import StreamingContext  \n",
    "from pyspark.streaming.kafka import KafkaUtils  \n",
    "from datetime import datetime\n",
    "\n",
    "batch_interval = 1\n",
    "window_length = 12 * batch_interval\n",
    "frequency = 6 * batch_interval\n",
    "\n",
    "sc = SparkContext(appName=\"CTR\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "directKafkaStream1 = KafkaUtils.createDirectStream(ssc, ['topic_click'], {\"metadata.broker.list\": 'ec2-52-9-120-149.us-west-1.compute.amazonaws.com:9092'})\n",
    "directKafkaStream2 = KafkaUtils.createDirectStream(ssc, ['topic_view'], {\"metadata.broker.list\": 'ec2-52-9-120-149.us-west-1.compute.amazonaws.com:9092'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd1 = directKafkaStream1.map(lambda x: x[1])\n",
    "# rdd1.pprint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = directKafkaStream2.map(lambda x: x[1])\n",
    "rdd2.pprint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd3 = rdd1.join(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# def mk_int(s):\n",
    "#     s = s.strip()\n",
    "#     return int(s) if s else 0\n",
    "# def tuple_sum(a, b):\n",
    "#     return [a[0]+b[0], a[1]+b[1]]\n",
    "# import mysql.connector\n",
    "# import MySQLdb\n",
    "# conn = MySQLdb.connect(host= \"52.9.120.149\",\n",
    "#                   user=\"name\",\n",
    "#                   passwd=\"password\",\n",
    "#                   db=\"modatabase\")\n",
    "# x = conn.cursor()\n",
    "# try:\n",
    "#    x.execute(\"\"\"INSERT INTO pet VALUES (%s,%s)\"\"\",(\"kelly\",\"mo\"))\n",
    "#    conn.commit()\n",
    "# except:\n",
    "#    conn.rollback()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def insert_into_mysql(clicks, total):\n",
    "#     import MySQLdb\n",
    "#     conn = MySQLdb.connect(host= \"52.9.120.149\",\n",
    "#                       user=\"name\",\n",
    "#                       passwd=\"password\",\n",
    "#                       db=\"modatabase\")\n",
    "#     x = conn.cursor()\n",
    "#     try:\n",
    "#         x.execute(\"\"\"INSERT INTO test_ctr VALUES (%s,%s)\"\"\",(datetime.now(), 100.0 * clicks / total))\n",
    "#         conn.commit()\n",
    "#         return (\"success ctr = \", 100.0 * clicks / total)\n",
    "#     except Exception, e:\n",
    "#         conn.rollback()\n",
    "#         return (str(e), 100.0 * clicks / total)\n",
    "#     conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ctr = lines.flatMap(lambda line: [line.split(\"\\t\")[-1]]) \\\n",
    "#     .map(lambda click: (\"ctr\", [mk_int(click), 1])) \\\n",
    "#     .reduceByKeyAndWindow(lambda a, b: tuple_sum(a, b), None, 12, 6) \\\n",
    "#     .flatMap(lambda row: insert_into_mysql(row[1][0], row[1][1]) )\n",
    "# ctr.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:23\n",
      "-------------------------------------------\n",
      "16925500\t2237188\t6\t1\t\t\n",
      "\n",
      "16925500\t2531181\t1\t3\t0.007261\t0\n",
      "\n",
      "16925500\t4317247\t8\t1\t\t\n",
      "\n",
      "16925500\t4669136\t7\t3\t0.001961\t0\n",
      "\n",
      "16925500\t23765387\t2\t2\t\t\n",
      "\n",
      "16925501\t6517544\t6\t1\t\t\n",
      "\n",
      "16925501\t6937457\t1\t3\t0.003086\t0\n",
      "\n",
      "16925501\t7428458\t8\t1\t\t\n",
      "\n",
      "16925501\t15764322\t7\t3\t0.000010\t0\n",
      "\n",
      "16925501\t32698262\t2\t1\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:24\n",
      "-------------------------------------------\n",
      "16940000\t16190594\t2\t1\t\t\n",
      "\n",
      "16940000\t17687043\t7\t3\t0.000010\t0\n",
      "\n",
      "16940000\t33211263\t1\t3\t0.000010\t0\n",
      "\n",
      "16940001\t3048116\t8\t1\t\t\n",
      "\n",
      "16940001\t23937494\t7\t1\t\t\n",
      "\n",
      "16940001\t26413523\t6\t1\t\t\n",
      "\n",
      "16940001\t34932059\t1\t3\t0.043111\t0\n",
      "\n",
      "16940001\t35184501\t2\t1\t\t\n",
      "\n",
      "16940002\t3880075\t7\t1\t\t\n",
      "\n",
      "16940002\t6067218\t8\t1\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:25\n",
      "-------------------------------------------\n",
      "16940676\t28620624\t8\t1\t\t\n",
      "\n",
      "16940677\t1477229\t7\t3\t0.001560\t0\n",
      "\n",
      "16940677\t3966071\t6\t1\t\t\n",
      "\n",
      "16940677\t15288242\t8\t1\t\t\n",
      "\n",
      "16940677\t29353540\t2\t1\t\t\n",
      "\n",
      "16940677\t34349082\t1\t3\t0.002915\t0\n",
      "\n",
      "16940678\t7246699\t8\t1\t\t\n",
      "\n",
      "16940678\t7342948\t1\t3\t0.010501\t0\n",
      "\n",
      "16940678\t25815876\t7\t3\t0.003437\t0\n",
      "\n",
      "16940678\t26404481\t6\t1\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:26\n",
      "-------------------------------------------\n",
      "16941649\t8989097\t8\t1\t\t\n",
      "\n",
      "16941649\t18147101\t1\t3\t0.034652\t0\n",
      "\n",
      "16941649\t25678701\t7\t3\t0.007642\t0\n",
      "\n",
      "16941650\t5381800\t1\t3\t0.090682\t0\n",
      "\n",
      "16941650\t19137057\t8\t1\t\t\n",
      "\n",
      "16941650\t19313551\t2\t2\t\t\n",
      "\n",
      "16941650\t34805754\t6\t1\t\t\n",
      "\n",
      "16941650\t35123196\t7\t3\t0.000914\t0\n",
      "\n",
      "16941651\t554277\t1\t3\t0.025439\t0\n",
      "\n",
      "16941651\t9312493\t6\t1\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:27\n",
      "-------------------------------------------\n",
      "16942786\t1877037\t1\t3\t0.002694\t0\n",
      "\n",
      "16942787\t364080\t2\t1\t\t\n",
      "\n",
      "16942787\t11685317\t8\t1\t\t\n",
      "\n",
      "16942787\t16965534\t6\t1\t\t\n",
      "\n",
      "16942787\t34349082\t1\t3\t0.001932\t0\n",
      "\n",
      "16942787\t35921165\t7\t3\t0.024543\t0\n",
      "\n",
      "16942788\t11047643\t7\t3\t0.001908\t0\n",
      "\n",
      "16942788\t29511493\t1\t3\t0.003831\t0\n",
      "\n",
      "16942789\t5663492\t1\t3\t0.009155\t0\n",
      "\n",
      "16942789\t6684085\t7\t3\t0.002605\t0\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:28\n",
      "-------------------------------------------\n",
      "16943881\t33399003\t1\t3\t0.002773\t0\n",
      "\n",
      "16943882\t3980338\t1\t3\t0.007444\t0\n",
      "\n",
      "16943882\t15480244\t7\t3\t0.003863\t0\n",
      "\n",
      "16943883\t11453510\t1\t3\t0.007246\t0\n",
      "\n",
      "16943883\t29022520\t7\t3\t0.004434\t0\n",
      "\n",
      "16943883\t30267391\t8\t1\t\t\n",
      "\n",
      "16943883\t32916374\t6\t1\t\t\n",
      "\n",
      "16943883\t34984778\t2\t2\t\t\n",
      "\n",
      "16943884\t864182\t1\t3\t0.003040\t0\n",
      "\n",
      "16943884\t19937876\t7\t3\t0.010062\t0\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:29\n",
      "-------------------------------------------\n",
      "16944668\t15920409\t2\t1\t\t\n",
      "\n",
      "16944668\t17599607\t7\t3\t0.007654\t0\n",
      "\n",
      "16944668\t28169181\t8\t1\t\t\n",
      "\n",
      "16944668\t30137540\t6\t1\t\t\n",
      "\n",
      "16944668\t33809385\t1\t3\t0.003218\t0\n",
      "\n",
      "16944669\t18542750\t2\t1\t\t\n",
      "\n",
      "16944669\t34380636\t1\t3\t0.005731\t0\n",
      "\n",
      "16944670\t1537147\t1\t3\t0.005658\t0\n",
      "\n",
      "16944670\t4117935\t7\t3\t0.002740\t0\n",
      "\n",
      "16944671\t8404495\t7\t3\t0.009165\t0\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:30\n",
      "-------------------------------------------\n",
      "16945504\t30921312\t6\t1\t\t\n",
      "\n",
      "16945504\t36625075\t2\t2\t\t\n",
      "\n",
      "16945505\t3910411\t8\t1\t\t\n",
      "\n",
      "16945505\t6718520\t1\t3\t0.009908\t0\n",
      "\n",
      "16945505\t12464980\t7\t3\t0.006790\t0\n",
      "\n",
      "16945505\t20293752\t2\t1\t\t\n",
      "\n",
      "16945505\t24208055\t6\t1\t\t\n",
      "\n",
      "16945507\t26901954\t1\t3\t0.002504\t0\n",
      "\n",
      "16945507\t28280403\t7\t3\t0.013943\t0\n",
      "\n",
      "16945508\t238588\t1\t3\t0.005169\t0\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:31\n",
      "-------------------------------------------\n",
      "16946493\t11586904\t1\t3\t0.003583\t0\n",
      "\n",
      "16946493\t19166032\t7\t1\t\t\n",
      "\n",
      "16946493\t21544345\t6\t1\t\t\n",
      "\n",
      "16946494\t9840503\t7\t3\t0.013373\t0\n",
      "\n",
      "16946494\t20752876\t1\t3\t0.007085\t0\n",
      "\n",
      "16946494\t25698058\t2\t2\t\t\n",
      "\n",
      "16946495\t21712085\t1\t3\t0.003621\t0\n",
      "\n",
      "16946495\t30623933\t7\t3\t0.002143\t0\n",
      "\n",
      "16946496\t15802412\t1\t3\t0.007483\t0\n",
      "\n",
      "16946496\t25885929\t2\t2\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:32\n",
      "-------------------------------------------\n",
      "16947592\t16114407\t1\t3\t0.018937\t0\n",
      "\n",
      "16947592\t23975192\t7\t1\t\t\n",
      "\n",
      "16947592\t34985638\t6\t1\t\t\n",
      "\n",
      "16947593\t6993194\t1\t3\t0.004262\t0\n",
      "\n",
      "16947594\t7444998\t2\t2\t\t\n",
      "\n",
      "16947594\t15363812\t1\t3\t0.032214\t0\n",
      "\n",
      "16947594\t24427862\t6\t1\t\t\n",
      "\n",
      "16947594\t32340432\t8\t1\t\t\n",
      "\n",
      "16947594\t36519660\t7\t3\t0.021503\t0\n",
      "\n",
      "16947595\t13644829\t2\t2\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:33\n",
      "-------------------------------------------\n",
      "16948683\t10662511\t7\t1\t\t\n",
      "\n",
      "16948683\t27230616\t2\t1\t\t\n",
      "\n",
      "16948683\t32608424\t8\t1\t\t\n",
      "\n",
      "16948683\t34606165\t1\t3\t0.003286\t0\n",
      "\n",
      "16948685\t1144156\t1\t3\t0.024249\t0\n",
      "\n",
      "16948685\t8752015\t7\t3\t0.013513\t0\n",
      "\n",
      "16948685\t20859905\t2\t2\t\t\n",
      "\n",
      "16948686\t6718582\t7\t3\t0.012345\t0\n",
      "\n",
      "16948686\t26166966\t1\t3\t0.000948\t0\n",
      "\n",
      "16948686\t33544334\t2\t2\t\t\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:34\n",
      "-------------------------------------------\n",
      "16949707\t29062946\t6\t1\t\t\n",
      "\n",
      "16949708\t10833685\t1\t3\t0.004561\t0\n",
      "\n",
      "16949708\t25769381\t7\t3\t0.003338\t0\n",
      "\n",
      "16949709\t5098366\t7\t3\t0.001837\t0\n",
      "\n",
      "16949709\t21692488\t1\t3\t0.034652\t0\n",
      "\n",
      "16949710\t9549836\t1\t3\t0.004078\t0\n",
      "\n",
      "16949710\t16615150\t6\t1\t\t\n",
      "\n",
      "16949710\t17267853\t7\t3\t0.001635\t0\n",
      "\n",
      "16949710\t31790669\t8\t1\t\t\n",
      "\n",
      "16949711\t24438982\t7\t3\t0.008585\t0\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-22 23:34:35\n",
      "-------------------------------------------\n",
      "16950491\t12429031\t7\t1\t\t\n",
      "\n",
      "16950491\t16891572\t1\t3\t0.000010\t0\n",
      "\n",
      "16950491\t33144160\t6\t1\t\t\n",
      "\n",
      "16950492\t3578262\t1\t3\t0.004134\t0\n",
      "\n",
      "16950492\t15382711\t2\t2\t\t\n",
      "\n",
      "16950492\t19937876\t7\t3\t0.009660\t0\n",
      "\n",
      "16950493\t894777\t6\t1\t\t\n",
      "\n",
      "16950493\t14729341\t7\t3\t0.003410\t0\n",
      "\n",
      "16950493\t25144740\t1\t3\t0.012559\t0\n",
      "\n",
      "16950493\t26846643\t2\t2\t\t\n",
      "\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o28.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 171, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1342, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/local/spark/python/pyspark/context.py\", line 968, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/lib/python2.7/dist-packages/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost, executor driver): java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f97a0d10b313>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark/python/pyspark/streaming/context.pyc\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 171, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1342, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/local/spark/python/pyspark/context.py\", line 968, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/lib/python2.7/dist-packages/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost, executor driver): java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "ssc.start()  \n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
